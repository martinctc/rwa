[{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Bootstrap confidence intervals represent major advancement Relative Weights Analysis, addressing long-standing methodological limitation. vignette provides comprehensive guidance using bootstrap methods rwa package statistical significance testing predictor importance.","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"the-statistical-challenge","dir":"Articles","previous_headings":"Why Bootstrap for RWA?","what":"The Statistical Challenge","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"noted Tonidandel et al. (2009): “difficulty determining statistical significance relative weights stems fact exact (small sample) sampling distribution relative weights unknown.” Traditional RWA provides point estimates relative importance lacks framework statistical inference. Bootstrap methods solve empirically estimating sampling distribution relative weights.","code":""},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"bootstrap-solution","dir":"Articles","previous_headings":"Why Bootstrap for RWA?","what":"Bootstrap Solution","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Bootstrap resampling: 1. Creates multiple samples original data 2. Calculates RWA bootstrap sample 3. Estimates confidence intervals distribution bootstrap results 4. Enables significance testing examining whether CIs include zero","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"simple-bootstrap-example","dir":"Articles","previous_headings":"Basic Bootstrap Analysis","what":"Simple Bootstrap Example","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Bootstrap analysis with 1000 samples result_bootstrap <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"),       bootstrap = TRUE,       n_bootstrap = 1000,       conf_level = 0.95)  # View results with confidence intervals result_bootstrap$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Raw.RelWeight.CI.Lower #> 1        hp     0.2321744           29.79691    -             0.18864625 #> 2       cyl     0.2284797           29.32274    -             0.17336836 #> 3      disp     0.2221469           28.50999    -             0.15772412 #> 4      gear     0.0963886           12.37037    +             0.04155014 #>   Raw.RelWeight.CI.Upper Raw.Significant #> 1              0.2811493            TRUE #> 2              0.2788206            TRUE #> 3              0.2804741            TRUE #> 4              0.1843592            TRUE"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"understanding-bootstrap-output","dir":"Articles","previous_headings":"Basic Bootstrap Analysis","what":"Understanding Bootstrap Output","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"bootstrap analysis enhances standard RWA output : Raw.RelWeight.CI.Lower/Upper: 95% confidence intervals raw weights Raw.Significant: Automatic significance flagging (CI doesn’t include zero)","code":"# Bootstrap-specific information cat(\"Bootstrap samples used:\", result_bootstrap$bootstrap$n_bootstrap, \"\\n\") #> Bootstrap samples used: 1000  # Detailed CI information print(result_bootstrap$bootstrap$ci_results$raw_weights) #> # A tibble: 4 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   #> 1 cyl                 1   0.173     0.279 bca       raw     #> 2 disp                2   0.158     0.280 bca       raw     #> 3 hp                  3   0.189     0.281 bca       raw     #> 4 gear                4   0.0416    0.184 bca       raw  # Identify significant predictors significant_vars <- result_bootstrap$result %>%   filter(Raw.Significant == TRUE) %>%   pull(Variables)  cat(\"Significant predictors:\", paste(significant_vars, collapse = \", \")) #> Significant predictors: hp, cyl, disp, gear"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"comprehensive-bootstrap-analysis","dir":"Articles","previous_headings":"Advanced Bootstrap Features","what":"Comprehensive Bootstrap Analysis","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"detailed analysis including focal variable comparisons:","code":"# Comprehensive bootstrap with focal variable comparison result_comprehensive <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\", \"wt\"),       bootstrap = TRUE,       comprehensive = TRUE,       focal = \"wt\",  # Compare other variables to weight       n_bootstrap = 500)  # Fewer samples for speed  # Access all bootstrap results names(result_comprehensive$bootstrap$ci_results) #> [1] \"raw_weights\"       \"random_comparison\" \"focal_comparison\""},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"bootstrap-parameters","dir":"Articles","previous_headings":"Advanced Bootstrap Features","what":"Bootstrap Parameters","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Key parameters bootstrap analysis: n_bootstrap: Number bootstrap samples (default: 1000) conf_level: Confidence level (default: 0.95) focal: Focal variable comparative analysis comprehensive: Enable additional bootstrap tests","code":"# Example with different parameters custom_bootstrap <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\"),       bootstrap = TRUE,       n_bootstrap = 2000,  # More samples for precision       conf_level = 0.99)   # 99% confidence intervals  custom_bootstrap$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Raw.RelWeight.CI.Lower #> 1       cyl     0.3837012           50.51586    -              0.2565545 #> 2      disp     0.3758646           49.48414    -              0.2433539 #>   Raw.RelWeight.CI.Upper Raw.Significant #> 1              0.4564831            TRUE #> 2              0.4607167            TRUE"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"important-considerations","dir":"Articles","previous_headings":"Rescaled Weight Confidence Intervals","what":"Important Considerations","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Rescaled weight confidence intervals interpreted caution due compositional data constraints. recommended formal statistical inference.","code":"# Rescaled CIs (use with caution) result_rescaled_ci <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\"),       bootstrap = TRUE,       include_rescaled_ci = TRUE,       n_bootstrap = 500)  # Note the warning message about interpretation result_rescaled_ci$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Raw.RelWeight.CI.Lower #> 1      disp     0.2793550           36.37966    -              0.2044890 #> 2       cyl     0.2723144           35.46279    -              0.2144884 #> 3        hp     0.2162184           28.15755    -              0.1548984 #>   Raw.RelWeight.CI.Upper Raw.Significant Rescaled.RelWeight.CI.Lower #> 1              0.3558447            TRUE                    30.46391 #> 2              0.3269149            TRUE                    30.26278 #> 3              0.2700555            TRUE                    20.63832 #>   Rescaled.RelWeight.CI.Upper #> 1                    42.88634 #> 2                    42.41383 #> 3                    35.88019"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"why-rescaled-cis-are-problematic","dir":"Articles","previous_headings":"Rescaled Weight Confidence Intervals","what":"Why Rescaled CIs Are Problematic","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Rescaled weights compositional data (sum 100%), creates dependencies variables. violates assumptions needed independent confidence intervals. Recommendation: Focus raw weight confidence intervals statistical inference.","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"diamond-price-analysis","dir":"Articles","previous_headings":"Real-World Applications","what":"Diamond Price Analysis","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Analyze diamond price drivers diamonds_subset <- diamonds %>%   select(price, carat, depth, table, x, y, z) %>%   sample_n(1000)  # Sample for faster computation  diamond_rwa <- diamonds_subset %>%   rwa(outcome = \"price\",       predictors = c(\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"),       bootstrap = TRUE,       applysigns = TRUE,       n_bootstrap = 500)  print(diamond_rwa$result) #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Sign.Rescaled.RelWeight #> 1     carat   0.257169175         29.1954988    +              29.1954988 #> 2         y   0.206439319         23.4363192    +              23.4363192 #> 3         z   0.205755304         23.3586655    +              23.3586655 #> 4         x   0.204112040         23.1721115    +              23.1721115 #> 5     table   0.004567369          0.5185171    +               0.5185171 #> 6     depth   0.002808932          0.3188880    -              -0.3188880 #>   Raw.RelWeight.CI.Lower Raw.RelWeight.CI.Upper Raw.Significant #> 1           0.2480932226            0.266629078            TRUE #> 2           0.2011322930            0.211372754            TRUE #> 3           0.2003312825            0.211292661            TRUE #> 4           0.1988540715            0.208972258            TRUE #> 5           0.0007763152            0.006628737            TRUE #> 6          -0.0006849704            0.003814884           FALSE"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"interpreting-results","dir":"Articles","previous_headings":"Real-World Applications","what":"Interpreting Results","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Focus on significant predictors (results are already sorted by importance) significant_drivers <- diamond_rwa$result %>%   filter(Raw.Significant == TRUE) %>%   select(Variables, Rescaled.RelWeight, Sign.Rescaled.RelWeight)  cat(\"Significant diamond price drivers (sorted by importance):\\n\") #> Significant diamond price drivers (sorted by importance): print(significant_drivers) #>   Variables Rescaled.RelWeight Sign.Rescaled.RelWeight #> 1     carat         29.1954988              29.1954988 #> 2         y         23.4363192              23.4363192 #> 3         z         23.3586655              23.3586655 #> 4         x         23.1721115              23.1721115 #> 5     table          0.5185171               0.5185171  cat(\"\\nModel R-squared:\", round(diamond_rwa$rsquare, 3)) #>  #> Model R-squared: 0.881"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"sample-size-guidelines","dir":"Articles","previous_headings":"Best Practices","what":"1. Sample Size Guidelines","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Check your sample size n_obs <- mtcars %>%    select(mpg, cyl, disp, hp, gear) %>%    na.omit() %>%    nrow()  cat(\"Sample size:\", n_obs) #> Sample size: 32 cat(\"\\nRecommended bootstrap samples:\", min(2000, n_obs * 10)) #>  #> Recommended bootstrap samples: 320  # Rule of thumb: At least 1000 bootstrap samples, more for smaller datasets"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"confidence-interval-interpretation","dir":"Articles","previous_headings":"Best Practices","what":"2. Confidence Interval Interpretation","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Examine CI characteristics ci_data <- result_bootstrap$bootstrap$ci_results$raw_weights print(head(ci_data)) #> # A tibble: 4 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   #> 1 cyl                 1   0.173     0.279 bca       raw     #> 2 disp                2   0.158     0.280 bca       raw     #> 3 hp                  3   0.189     0.281 bca       raw     #> 4 gear                4   0.0416    0.184 bca       raw  # Assess precision ci_analysis <- ci_data %>%   mutate(     significant = ci_lower > 0 | ci_upper < 0,     ci_width = ci_upper - ci_lower,     precision = case_when(       ci_width < 0.05 ~ \"High precision\",       ci_width < 0.15 ~ \"Medium precision\",        TRUE ~ \"Low precision\"     )   )  print(ci_analysis) #> # A tibble: 4 × 9 #>   variable weight_index ci_lower ci_upper ci_method ci_type significant ci_width #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   <lgl>          <dbl> #> 1 cyl                 1   0.173     0.279 bca       raw     TRUE          0.105  #> 2 disp                2   0.158     0.280 bca       raw     TRUE          0.123  #> 3 hp                  3   0.189     0.281 bca       raw     TRUE          0.0925 #> 4 gear                4   0.0416    0.184 bca       raw     TRUE          0.143  #> # ℹ 1 more variable: precision <chr>"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"bootstrap-method-selection","dir":"Articles","previous_headings":"Best Practices","what":"3. Bootstrap Method Selection","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"package automatically selects best available bootstrap CI method: BCA (Bias-Corrected Accelerated) - Preferred possible Percentile - Fallback BCA fails Basic bootstrap - Final fallback option","code":"# Check which methods were used ci_methods <- result_bootstrap$bootstrap$ci_results$raw_weights %>%   count(ci_method)  print(ci_methods) #> # A tibble: 1 × 2 #>   ci_method     n #>   <chr>     <int> #> 1 bca           4"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"bootstrap-speed-tips","dir":"Articles","previous_headings":"Performance Considerations","what":"Bootstrap Speed Tips","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# For large datasets or many predictors, consider:  # 1. Reduce bootstrap samples for initial exploration quick_result <- mtcars %>%   rwa(outcome = \"mpg\",        predictors = c(\"cyl\", \"disp\"),        bootstrap = TRUE,        n_bootstrap = 500)  # Faster  # 2. Use comprehensive analysis only when needed # comprehensive = TRUE adds computational overhead  # 3. Consider parallel processing for very large analyses # (not currently implemented but could be future enhancement)"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"memory-usage","dir":"Articles","previous_headings":"Performance Considerations","what":"Memory Usage","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Bootstrap objects can be large - access specific components str(result_bootstrap$bootstrap, max.level = 1) #> List of 6 #>  $ boot_object  :List of 11 #>   ..- attr(*, \"class\")= chr \"boot\" #>   ..- attr(*, \"boot_type\")= chr \"boot\" #>  $ ci_results   :List of 1 #>  $ n_bootstrap  : num 1000 #>  $ conf_level   : num 0.95 #>  $ comprehensive: logi FALSE #>  $ focal        : NULL  # For memory efficiency, extract only needed results ci_summary <- result_bootstrap$bootstrap$ci_results$raw_weights %>%   select(variable, ci_lower, ci_upper, ci_method)  print(ci_summary) #> # A tibble: 4 × 4 #>   variable ci_lower ci_upper ci_method #>   <chr>       <dbl>    <dbl> <chr>     #> 1 cyl        0.173     0.279 bca       #> 2 disp       0.158     0.280 bca       #> 3 hp         0.189     0.281 bca       #> 4 gear       0.0416    0.184 bca"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"common-bootstrap-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Common Bootstrap Issues","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# 1. Check for perfect multicollinearity cor_check <- mtcars %>%   select(cyl, disp, hp, gear) %>%   cor()  # Look for correlations = 1.0 (excluding diagonal) perfect_cor <- which(abs(cor_check) == 1 & cor_check != diag(diag(cor_check)), arr.ind = TRUE)  if(length(perfect_cor) > 0) {   cat(\"Perfect multicollinearity detected - remove redundant variables\") } else {   cat(\"No perfect multicollinearity detected\") } #> No perfect multicollinearity detected  # 2. Ensure adequate sample size min_sample_size <- 5 * length(c(\"cyl\", \"disp\", \"hp\", \"gear\"))  # 5 obs per predictor actual_sample_size <- nrow(na.omit(mtcars[c(\"mpg\", \"cyl\", \"disp\", \"hp\", \"gear\")]))  cat(\"\\nMinimum recommended sample size:\", min_sample_size) #>  #> Minimum recommended sample size: 20 cat(\"\\nActual sample size:\", actual_sample_size) #>  #> Actual sample size: 32"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"standard-reporting-format","dir":"Articles","previous_headings":"Reporting Bootstrap Results","what":"Standard Reporting Format","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"reporting bootstrap RWA results, include: Sample size missing data handling Bootstrap parameters (number samples, confidence level) CI method used (BCA, percentile, basic) Significant predictors confidence intervals Model fit (R-squared)","code":""},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"example-report","dir":"Articles","previous_headings":"Reporting Bootstrap Results","what":"Example Report","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"","code":"# Generate a summary report report_data <- result_bootstrap$result %>%   filter(Raw.Significant == TRUE) %>%   arrange(desc(Rescaled.RelWeight)) %>%   select(Variables, Rescaled.RelWeight, Raw.RelWeight.CI.Lower, Raw.RelWeight.CI.Upper)  cat(\"Relative Weights Analysis Results\\n\") #> Relative Weights Analysis Results cat(\"=================================\\n\") #> ================================= cat(\"Sample size:\", result_bootstrap$n, \"\\n\") #> Sample size: 32 cat(\"Bootstrap samples:\", result_bootstrap$bootstrap$n_bootstrap, \"\\n\") #> Bootstrap samples: 1000 cat(\"Model R-squared:\", round(result_bootstrap$rsquare, 3), \"\\n\\n\") #> Model R-squared: 0.779 cat(\"Significant Predictors:\\n\") #> Significant Predictors: print(report_data) #>   Variables Rescaled.RelWeight Raw.RelWeight.CI.Lower Raw.RelWeight.CI.Upper #> 1        hp           29.79691             0.18864625              0.2811493 #> 2       cyl           29.32274             0.17336836              0.2788206 #> 3      disp           28.50999             0.15772412              0.2804741 #> 4      gear           12.37037             0.04155014              0.1843592"},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Bootstrap Methods RWA: Tonidandel, S., LeBreton, J. M., & Johnson, J. W. (2009). Determining statistical significance relative weights. Psychological Methods, 14(4), 387-399. General Bootstrap Theory: Efron, B., & Tibshirani, R. J. (1993). introduction bootstrap. Chapman & Hall/CRC. Compositional Data Analysis: Aitchison, J. (1986). statistical analysis compositional data. Chapman & Hall.","code":""},{"path":"https://martinctc.github.io/rwa/articles/bootstrap-confidence-intervals.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Bootstrap Confidence Intervals for Relative Weights Analysis","text":"Bootstrap confidence intervals provide robust solution statistical inference Relative Weights Analysis. following guidelines vignette, researchers can: Determine statistical significance predictor importance Report confidence intervals appropriate interpretations Avoid common pitfalls bootstrap analysis Apply best practices reliable results bootstrap functionality rwa package represents significant advancement making RWA complete tool exploratory analysis confirmatory research.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"evaluation-of-the-tonidandel-lebreton-relative-weights-analysis-method","dir":"Articles","previous_headings":"","what":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Relative Weights Analysis (RWA), articulated Scott Tonidandel James LeBreton, statistical technique designed determine relative importance predictor variables regression model, especially predictors intercorrelated (multicollinearity). essence, RWA partitions model’s total explained variance (_R_²) among predictors show much contributes predicting outcome. method inspired earlier work variance partitioning (e.g. Lindeman et al., 1980) addresses shortcomings traditional regression metrics multicollinearity. evaluate current validity RWA handling multicollinearity, compare methods (highlighting excels falls short), discuss practical considerations scenarios use.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"what-is-relative-weights-analysis-and-how-does-it-address-multicollinearity","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method","what":"What is Relative Weights Analysis and How Does it Address Multicollinearity?","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Relative Weights Analysis (RWA) (also known relative importance analysis) technique transforms original predictors new set orthogonal (uncorrelated) variables uses apportion variance outcome back predictor. result set weights predictor sum model’s total R² (explained variance). words, weight represents portion outcome variance uniquely attributable predictor, taking account overlap predictors. definition “relative importance” considers predictor’s contribution combination others, giving holistic measure simple regression coefficient.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"why-is-this-useful-for-multicollinearity","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > What is Relative Weights Analysis and How Does it Address Multicollinearity?","what":"Why is this useful for multicollinearity?","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"standard multiple regression, predictors highly correlated, usual indicators importance (like standardized beta coefficients p-values) can misleading. Regression coefficients may become unstable, change signs, appear nonsignificant due shared variance among predictors. example, predictor strongly correlated outcome might get near-zero even negative beta another collinear predictor “soaks ” variance model. Tonidandel & LeBreton (2011) note commonly used indices “fail appropriately partition variance predictors correlated”. RWA directly tackles partitioning overlapping variance principled way: attributes shared (collinear) variance predictors predictors proportion structure outcome. effect, RWA untangles multicollinearity show variable’s true impact. means even two predictors strongly correlated , can still receive substantial relative weight contributes predicting Y. Crucially, weights non-negative sum R², making interpretable percentage explained variance (e.g. weight 0.30 model R²=0.60 means predictor accounts 0.30/0.60 = 50% explained variance). technical terms, Johnson (2000) algorithm RWA works follows: performs eigenvalue decomposition predictor correlation matrix create uncorrelated principal components, regresses outcome components. resulting regression coefficients component loadings combined compute predictor’s share variance outcome. procedure yields weights virtually identical obtained averaging predictor’s incremental R² contribution possible subsets predictors (logic used dominance analysis). However, RWA without brute-force search subsets, ’s computationally efficient. using orthogonal basis, RWA preserves interpretability original predictors (weight maps back original variable) circumventing multicollinearity problem model fitting step. Essentially, achieves principal components regression might — decorrelating predictors — translates result back variance attributed original variable.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"is-rwa-still-a-valid-approach-for-evaluating-predictor-power-under-multicollinearity","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > What is Relative Weights Analysis and How Does it Address Multicollinearity?","what":"Is RWA still a valid approach for evaluating predictor power under multicollinearity?","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Yes – RWA remains widely accepted useful method purpose. specifically designed situations correlated predictors “particularly useful predictors correlated since deals issues multicollinearity.”. Since introduction, numerous studies validated RWA indeed gives fair assessment variable’s importance even high-correlation settings. practice, RWA better partitions variance standard regression: researchers applying found reveals important predictors regression beta weights obscured due multicollinearity. example, one organizational study, “serial tactics” variable appeared unimportant standard regression (beta non-significant another variable collinear), RWA showed actually explained meaningful variance outcome, supporting theoretical importance. illustrates RWA’s value preserving predictor’s contribution. said, RWA magic bullet completely nullifies multicollinearity concerns (limitations later). two predictors essentially measuring underlying construct, RWA correctly indicate together contribute prediction, split credit , potentially making look less important individually. extreme cases (e.g. r ≈ 0.98 two variables), individual weights might small nearly equal, one’s predictive power redundant . statistically appropriate – reflects one alone can job – user must recognize pair whole may important even alone modest weight. short, RWA handles multicollinearity well partitions variance consistently informatively naive methods. remains valid recommended approach evaluating predictor contributions multicollinearity, provided interpret results proper theoretical context (e.g. recognizing two variables essentially interchangeable measures one factor).","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"strengths-of-rwa-relative-to-other-methods","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method","what":"Strengths of RWA Relative to Other Methods","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA developed address deficiencies standard approaches, several strengths advantages methods gauging predictor importance:","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"superior-to-standard-regression-coefficients-under-multicollinearity","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Strengths of RWA Relative to Other Methods","what":"1. Superior to Standard Regression Coefficients (under multicollinearity)","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"typical regression analysis, researchers often look magnitude significance standardized beta coefficients judge importance. However, can misleading predictors correlated. variable’s beta reflects unique contribution holding others constant, presence multicollinearity might severely underestimate variable shares variance others. Tonidandel & LeBreton observed many authors mistakenly labeled variables “meaningful” based nonsignificant betas, fact variables explain variance criterion. RWA, contrast, gives credit predictor variance shares outcome jointly variables. often reveals predictor substantial importance even regression coefficient suppressed multicollinearity. Indeed, Courville & Thompson (2001) found cases predictor near-zero beta actually one top contributors R² – something RWA clearly show (since variable’s relative weight relatively large, reflecting combined effect). short, RWA equitable interpretable metric importance raw beta weights predictors overlap. considers direct indirect (shared) effects, whereas beta reflects direct unique portion. makes RWA far superior relying p-values coefficients semi-partial correlations assessing “predictors matter” correlated systems.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"better-than-stepwise-or-bivariate-ranking","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Strengths of RWA Relative to Other Methods","what":"2. Better than Stepwise or Bivariate Ranking","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Sometimes analysts try circumvent multicollinearity using stepwise regression looking bivariate correlations outcome. approaches drawbacks RWA avoids. Stepwise methods arbitrarily pick one set collinear variables drop others, might lead conclude dropped variables unimportant. reality, might just important, contribution redundant chosen variable. RWA show collinear variables high relative weights (indicating account variance considered). Unlike stepwise selection, RWA throw away information; tells much predictor contributes given whole set. Tonidandel & LeBreton caution, relative weights used select eliminate variables – correct model decided first, RWA helps interpret . Compared simple bivariate correlations Y, RWA also superior. high correlation may exaggerate variable’s standalone importance, low one may hide variable acts combination others. RWA puts predictors level playing field evaluating together. one primer notes, predictors uncorrelated, RWA simple squared correlations give ranking, predictors correlated (usual case), “elaborate methods” like RWA needed properly assess importance. essence, RWA automatically accounts suppressor effects synergy among predictors simple correlations stepwise methods can miss.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"advantages-over-principal-component-or-factor-approaches","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Strengths of RWA Relative to Other Methods","what":"3. Advantages over Principal Component or Factor Approaches","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"common strategy handle multicollinearity perform principal component analysis (PCA) factor analysis predictors, use uncorrelated components regression. addresses numerical multicollinearity problem, buries identity individual predictors – end composite factors interpretable original variables. contrast, RWA uses orthogonalization behind scenes ultimately returns original predictor scale. predictor gets weight units variance explained (e.g. R² share). Thus, RWA retains interpretability: can say “Variable X accounts 20% explainable variance Y,” straightforward. PCA, might find principal component important, component might mix several original variables – actionable intuitive. Additionally, PCA-based regression typically still won’t tell much variance original variable responsible ; additional calculations map component importance back variables. RWA essentially mapping (via Johnson’s formula). , RWA can seen direct way get “interpretable principal components” importance. ’s worth noting predictors extremely multicollinear due measuring construct, one combine priori (e.g. average ) create single predictor – decision outside scope statistical measures. RWA’s job measure contributions predictors given, way leverages techniques like PCA keeping results terms original features.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"comparable-outcome-to-dominance-analysis-with-greater-speed-and-extras","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Strengths of RWA Relative to Other Methods","what":"4. Comparable Outcome to Dominance Analysis, with Greater Speed and Extras","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Dominance analysis (Shapley value regression) often considered gold-standard determining relative importance exhaustively considers every subset predictors. RWA’s greatest achievement produces almost result dominance analysis, far efficiently. Monte Carlo research LeBreton, Ployhart & Ladd (2004) found rankings predictors Johnson’s relative weights essentially general dominance weights across variety conditions. correlation two methods’ importance scores typically >0.99 simulations real datasets. fact, two predictors, proven Johnson’s RWA Shapley value exactly coincide. Thus, one doesn’t lose meaningful accuracy using RWA. Meanwhile, time saved enormous: dominance analysis requires examining 2^p models (p predictors). p=10, ’s 1,024 subset regressions; p=20, 1 million; p=30, 1 billion subset models. quickly becomes computationally infeasible. Johnson’s RWA bypasses combinatorial explosion solving set equations instead. difference dramatic. example, one practitioner noted model 30 predictors take “days run” Shapley regression, whereas RWA computes “almost instantly” modern computer. RWA’s negligible computation time means can include many predictors theory data allow (within reason) without worrying algorithmic blow-. Another strength RWA easily provides analytical tools like confidence intervals significance tests weights. RWA closed-form solution, researchers like Johnson (2004) Tonidandel et al. (2009) derived methods estimate standard errors weights (often using bootstrapping). lets test weight significantly greater zero (.e. predictor contributes significantly R²) , something straightforward dominance analysis except via intensive bootstrapping. Tonidandel et al. (2009) even provided technique test one predictor’s weight significantly different another’s examining bootstrap distributions. statistical comparison capabilities now built tools like RWA Web. contrast, dominance analysis typically requires custom resampling methods assess variability. RWA also easier extend modeling contexts: instance, Tonidandel & LeBreton (2010) showed apply variant RWA logistic regression (traditional R² doesn’t exist, one can use analogues) , LeBreton & Tonidandel (2008) extended multivariate criterion (multiple outcomes) cases. Dominance analysis can extended , becomes even computationally arduous settings. summary, RWA gives benefits rigorous relative importance measure without downsides computational complexity, adds convenient features (like significance testing adaptability non-OLS models) make practical users.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"proven-in-practice-and-accessible","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Strengths of RWA Relative to Other Methods","what":"5. Proven in Practice and Accessible","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Another soft “strength” RWA embraced various fields reliable tool. ’s just theoretical construct; many researchers found helpful real data problems. example, organizational psychologists use determine employee job factors important predicting performance satisfaction, without misled intercorrelations among factors. Marketing scientists use “key driver analysis” figure product attributes drive overall customer satisfaction, attributes ratings tend move together. fact, 2017 tutorial calls relative weights analysis “way exploring relative importance predictors” demonstrates utility psychological research scenarios multicollinear predictors. uptake, now user-friendly software packages: R package relaimpo (Grömping, 2006) newer one rwa implement Johnson’s method, aforementioned RWA Web tool provides point--click interface. words, method’s strengths recognized point ’s readily available analysts doesn’t require hand-coding. broad usage confirms method remains highly relevant advantageous analyzing predictor importance multicollinearity.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"limitations-and-comparisons-where-rwa-falls-short-or-other-methods-prevail","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method","what":"Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA many strengths, important understand limitations compares alternative methods scenarios might top choice. single technique best respects, RWA exception. Key points consider include:","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"not-a-replacement-for-theory-or-causal-insight","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"1. Not a Replacement for Theory or Causal Insight","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA ultimately descriptive, variance-partitioning tool. tells us “much prediction attributable X” imply causation policy. Tonidandel & LeBreton (2011) stress relative weights “causal indicators thus necessarily dictate course action”. example, one predictor highest weight, means important regression sense; doesn’t automatically mean changing predictor largest effect outcome (causality manipulability separate issues). RWA used enhance interpretation regression models, decisions still guided substantive theory. aids theory building correctly identifying predictors matter , can refine theoretical models , tell predictor important unmeasured confounder play. short, supplements replace need careful theoretical reasoning.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"dominance-analysis-vs--rwa-a-matter-of-precision-vs--practicality","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"2. Dominance Analysis vs. RWA – a matter of precision vs. practicality:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Dominance analysis (DA) alternative , principle, provides even information RWA. RWA gives predictor one number (weight), DA breaks predictor’s contributions every subset predictors. DA, can derive additional nuanced concepts like complete dominance, conditional dominance, general dominance (examine importance different subset sizes). can surface insights suppressor variables – predictors low importance increase another variable’s importance combination. Tonidandel & LeBreton acknowledge detailed information interest, dominance analysis preferred RWA. words, RWA compresses information single summary per predictor, whereas DA can tell , example, predictor dominates B subset sizes, important combined C, etc. However, situations (like identifying complex suppression patterns) relatively specialized. research scenarios, single importance score per variable sufficient interpretable. authors note cases many predictors significance testing weights needed, RWA practical thus often preferred dominance analysis. Moreover, computing power grows, one might attempt DA often, even today, dominance analysis can become unwieldy large predictor set complex models. recent methods article (Braun et al., 2019) pointed although modern computing allows DA typical regression sizes, sampling variability DA estimates (due finite sample) can make ranks unstable, requiring confidence intervals caution just like RWA. summary: absolute mathematical rigor needed p small, dominance analysis considered “superior” (directly implements definition relative importance via subset contributions). practice, RWA’s near equivalence added convenience usually outweigh negligible theoretical loss information. Indeed, recent authors suggest performing DA feasible using RWA cases DA computationally difficult (e.g. multivariate outcomes). ’s telling even critics RWA concede applications results similar, differences become theoretical nature. One specific critique Thomas et al. (2014) argued RWA’s mathematical derivation flaws contrived cases lead “distorted inferences” compared DA. However, also “warned” using RWA acknowledging approaches give similar results practical data geometrically identical two predictors. effect, critique underscores dominance analysis conceptually pure method, provide evidence RWA mis-ranks important predictors realistic scenarios. Thus, methodologists continue view RWA legitimate technique, using dominance analysis cross-check convenient.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"multicollinearity-still-matters-if-it-means-redundancy","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"3. Multicollinearity still matters if it means redundancy","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"mentioned earlier, RWA isn’t panacea extreme multicollinearity stemming redundant predictors. partition variance among highly collinear predictors, might yield misleadingly low weights – misleading sense one might incorrectly infer variable unimportant, whereas truth set important share contribution. Tonidandel & LeBreton explicitly caution: “One mistakenly held belief importance weights solve problem multicollinearity…. two predictors highly correlated tap underlying construct, resulting importance weights can misleading…. One need consider dropping one highly multicollinear variables forming composite.”. go say ’s absolute statistical cutoff “much” multicollinearity – ’s theoretical question whether two variables essentially measuring thing. merely related distinct (e.g. income education correlated identical), RWA partition variance appropriately actually performs much better regression coefficients conditions; duplicitous (e.g. income income euros), RWA simply split income variance , making look half important. , onus researcher diagnose cases redundant predictors. scenarios theoretical redundancy, RWA “inferior” per se – correctly reflects individually variables add less overlap – interpretation can trip unwary users. remedy straightforward: either combine variables one composite predictor acknowledge weights considered sum. limitation essentially shared importance method: even dominance analysis show two redundant predictors roughly half total combined importance, one similarly decide drop merge .","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"sample-size-and-sampling-error","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"4. Sample Size and Sampling Error","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA, like multiple regression, relies sample estimates correlations can unstable small samples high predictor--sample ratio. doesn’t inherently “fix” issue little data. Tonidandel & LeBreton (2011) note importance weights derived given sample can differ true population values due sampling error measurement error, just regression coefficients can. Bootstrapping helps quantify uncertainty producing confidence intervals weights , “bootstrapping overcome inherent limitations associated small sample size”. practice, one ensure sample large enough support stable estimation multiple regression model first place. rule thumb often used: least 10–15 observations per predictor (generally N > 100) regression-type analyses. violate , RWA might yield weights, confidence intervals huge conclusions uncertain. comparison methods, unique weakness RWA – method assessing importance suffer insufficient data. However, one argue simpler methods (like looking bivariate correlations) might “work” smaller N require estimating fewer parameters. instance, internal analytics note pointed correlation analysis can sometimes done N ~ 30, whereas full regression many predictors might demand N > 100 get reliable estimates. cases, sample size increased, analyst might opt simpler exploratory measures (acknowledging limitations) focus smaller set predictors. RWA inferior much data-hungry simple correlation approach. bottom line : adequate sample, RWA excels; small sample, method perform well, one must cautious. Using bootstrap confidence intervals even comparing weights “random noise” variable (technique suggested Tonidandel et al., 2009) can help judge whether weight significantly random chance produce.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"interpretation-of-weights-communication-challenges","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"5. Interpretation of Weights – Communication Challenges","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"easier interpret many alternatives, relative weights might still confuse audiences familiar . example, might mistakenly think higher weight means variable higher beta larger causal effect. falls analyst properly explain “Variable X relative importance 0.30, meaning accounted 30% explained variance Y model.” interpretation – essentially type “importance percentage” – tends intuitive explained. fact, rescaled relative weights (expressed percentages R²) often reported easy communication. However, one must avoid overinterpreting exact percentages precise population; subject confidence intervals. Also, weights always sum R², compositional nature (increase one weight means decrease others, R² fixed). means typically shouldn’t treat differences weights meaningful unless statistically significant large. instance, one predictor 25% another 20% R², might significantly different given sampling error. Thus, RWA users embrace statistical tests least bootstrap intervals compare weights, rather blindly ranking tiny differences – caution applies importance metric. Recent research (Braun et al., 2019) found rank ordering predictors importance can sampling error, recommended techniques like reporting confidence intervals ranks performing paired comparisons weights. , one limitation people may tempted -interpret rank ordering without regard overlap intervals. remedy good practice reporting: give weights, perhaps give standard errors CIs, highlight clear distinctions.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"when-other-methods-might-be-considered-superior","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Limitations and Comparisons: Where RWA Falls Short or Other Methods Prevail","what":"6. When other methods might be considered “superior”","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Aside dominance analysis (discussed ), methods contexts mention. one’s goal purely predictive accuracy rather interpretability, methods like ridge regression lasso regression handle multicollinearity shrinking selecting predictors, possibly yielding better prediction model. methods provide straightforward variance decomposition original variables; answer different question (improving prediction regularization) can drop correlated variables entirely. Therefore, terms “evaluating predictive power variables,” RWA directly informative lasso feature selection – RWA tell two collinear variables together explain, say, 40% variance (split 20%/20%), whereas lasso might simply keep one discard , telling nothing discarded variable’s potential importance. Another approach random forest machine-learning variable importance measures. can capture complex non-linear importance also handle correlated predictors extent (random forest’s permutation importance, example, can interpreted much prediction error increases variable permuted). Random forests tend indicate one variable correlated group important others low importance (since one used splits, others add little new). might seen advantage (picks representative) disadvantage (doesn’t inform overlap) depending goal. 2023 article recommends using dominance analysis random forest importance robustly identify key predictors. However, ML methods “black box” sense importance measured model’s terms (like Gini impurity reduction --bag error), percentage variance RWA. , question specifically variance explained relative contribution linear model, RWA interpretable aligned goal ML feature importance. hand, one suspects non-linear effects interactions drive importance, RWA (based linear model) might miss , case methods like random forests boosted trees considered complementary. summary, RWA robust method assessing variable importance presence multicollinearity, infallible universally best every purpose. “inferiors” mainly naive methods designed improve upon (raw betas, stepwise routines, etc.), clearly outperforms delivering insight. “superior equal peers” dominance analysis (essentially equivalent result, albeit effort) , specific aims, approaches like regularization machine learning (serve different objectives). RWA’s limitations generally well-understood can managed user: ensure solid theoretical model, cautious essentially duplicate predictors, use RWA results part bigger interpretive picture (alongside coefficient estimates, etc.), communicate findings appropriate nuance (e.g. include CIs, emphasize proportions variance, causal effect sizes). Stadler et al. (2017) concluded primer, relative weights dominance analysis provide valuable additional information beyond classical regression, fix problems used supplements regression rather replacements. perspective nicely captures RWA’s role: important tool toolkit, best used awareness assumptions concert analyses.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"considerations-and-best-practices-for-using-rwa","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method","what":"Considerations and Best Practices for Using RWA","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"researcher analyst decides use Tonidandel & LeBreton RWA method evaluate predictor importance, several practical considerations scenarios bear mind order get method:","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"ensure-the-model-is-well-specified","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"1. Ensure the Model is Well-Specified:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA assumes chosen set predictors analyze (.e. “correct” model). tell predictors include – determine based theory, prior research, model selection techniques. noted, RWA isn’t variable selection can actually mislead used way (e.g., variable small weight big model might still worth keeping ’s theoretically critical weight grow smaller model). , first specify regression model (perhaps checking multicollinearity diagnostics, transforming variables needed, etc.), apply RWA interpret model. ’re unsure including variable high correlation another, consider purpose: measure something similar need one, combine pick one running RWA. conceptually important distinct constructs, include let RWA partition influence – just ready interpret result (maybe weights split). example, education income job performance model correlate 0.8, decide ’re distinct predictors one proxy socioeconomic status; latter, combine one index. decision outside RWA’s scope. RWA presupposes set predictors given meaningful.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"check-data-and-assumptions","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"2. Check Data and Assumptions:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"RWA shares basic assumptions multiple regression: linear relationships predictors outcome (unless explicitly include polynomial terms interactions), reliable measurement variables. Extreme multicollinearity (e.g. variables linear combinations ) cause computational issues (singular matrices). get warnings correlation matrix singular near-singular, must address (removing combining collinear variables) trusting weights. One good practice inspect correlation matrix predictors high correlations (say > 0.9). found, make conscious decision handle variables. Also, handle missing data (RWA typically uses listwise deletion default, like regression ) – ensure missingness isn’t introducing bias. Outliers can affect regression thus RWA; data extreme outliers distort correlations, consider remedying (via robust methods transformations) prior RWA. Essentially, treat RWA regression analysis terms preparing clean, appropriate input data.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"adequate-sample-size","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"3. Adequate Sample Size:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"’s worth emphasizing – use RWA datasets N large enough support model. small sample many predictors, weights large uncertainty. cases, bootstrap procedure highly recommended get confidence intervals weights. Many RWA software implementations can bootstrap easily (example, Tonidandel & LeBreton’s RWA web tool R packages allow specifying number bootstrap resamples). common practice using 500 1,000 bootstrap samples derive 95% confidence intervals weight. predictor’s interval includes low values (relative others relative zero), cautious declaring important. Tonidandel et al. (2009) also suggest clever bootstrap test: include completely random “noise” variable model compute RWA. real predictors weights significantly larger noise variable’s weight, ’s evidence truly contribute beyond chance. addresses issue relative weights, proportions R², never exactly zero sample (even noise get tiny share). bottom line – predictors , larger sample get stable estimates. say 10 predictors, sample 50 quite low; get data, least tentative results. Conversely, big data scenarios (N thousands), RWA can shine – give precise estimates importance.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"interpreting-the-output-weights","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"4. Interpreting the Output (Weights):","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"run RWA, get “Raw Relative Weights” predictor, sum model’s R². Often also column weights rescaled percentages R² (summing 100%) convenience. interpret measures effect size importance. weight 0.10 (R² = 0.50, .e. 20%) might considered moderately important; something like 0.30 0.50 (60%) important. ’s rigid cut-constitutes “high” importance – ’s relative. One good practice rank predictors weights see natural drop-point clustering. example, might find two top predictors weights around 0.25 , third 0.15, fourth 0.05, etc. tells two predictors clearly dominate importance. However, always refer uncertainty: two weights close (say 0.25 vs 0.22), check difference likely real just sampling noise. ’ve bootstrapped, see 95% CIs overlap. , might say predictors tied importance statistically. one’s interval wholly ’s, can confidently say predictor important B. Additionally, look signs relationships provided. default, relative weights concerned magnitude explained variance (inherently non-negative). implementations, like rwa R package, offer signed version sign predictor’s correlation outcome attached weight context – e.g. “X accounts 15% variance effect positive (higher X -> higher Y)”. can useful report, stakeholders often ask just “important ?” “direction influence outcome?”. , best practice: report relative weights alongside sign predictor’s correlation regression coefficient. example: “Education relative importance 0.18 (36% R²) predicting income, positively related income (higher education associated higher income).” gives full picture: importance direction.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"comparing-weights-and-groups","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"5. Comparing Weights and Groups","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"analysis calls comparing predictors’ importances within different groups models, RWA procedures . instance, might run RWA separately males females see pattern importance differs. Tonidandel & LeBreton (2015) discuss methods statistically compare weights across groups (essentially pooling bootstraps using tests differences). using web tool software supports , can test, say, whether “education significantly stronger predictor job performance males females.” advanced use-case shows flexibility RWA – something cumbersome dominance analysis methods. comparisons, ensure models comparable groups sufficient sample .","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"use-case-scenarios-when-to-use-rwa","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"6. Use Case Scenarios – when to use RWA","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"consider using RWA whenever face regression analysis understanding relative contribution correlated predictors important goals. Classic scenarios:","code":"- **Key Driver Analysis:** In marketing or HR analytics, you have many survey items or factors that all correlate (e.g. many aspects of customer satisfaction, or many employee engagement factors) and you want to know which ones matter most for an outcome (like loyalty or performance). RWA is ideal here because it will handle the inter-correlations among those drivers and yield a list of the top drivers by importance. It's been used widely in these domains to present results as a bar chart of percentage contributions, which is easy for managers to digest. - **Multicollinear Predictors in Scientific Research:** In fields like ecology, psychology, or economics, you may have variables that naturally occur together (e.g. various climate variables, or socioeconomic indicators). If you include them in a regression, you'll want to partition their effects. RWA has been recommended in psychology precisely for such situations – e.g., intelligence research often measures different cognitive abilities that inter-correlate; RWA can tell which ability accounts for more variance in academic performance, for instance. - **Suppressor Situations:** If you suspect suppressor effects (where a variable increases another's predictive validity by being included), RWA can help illuminate that. A suppressor variable might not correlate strongly with Y itself but helps remove irrelevant variance from another predictor, thereby boosting that predictor's beta. In RWA, the suppressor could still get a moderate weight because in combination it contributes to R², and the combination effects are inherently counted. Dominance analysis could pinpoint which subsets show suppression, but RWA will at least not ignore the joint contribution. - **Logistic Regression \"Pseudo-R²\" analysis:** If your outcome is binary (yes/no) or categorical, and you use logistic regression, you don't have an exact R², but you can still use analogues (like Nagelkerke R² or just treat it as variance in log-odds explained). Tonidandel & LeBreton (2010) extended RWA to logistic regression by deriving weights using an analogous approach. If you have software that supports it (some implementations do), you can interpret importance in logistic models similarly. For example, in predicting employee turnover (left vs stayed), where predictors might be job satisfaction, pay, tenure, etc., RWA can tell which contributes most to the prediction of turnover, even though it's a non-linear model. - **Multiple Outcomes:** If you're interested in which predictors are important across multiple outcomes (say you have a set of related outcome variables), the multivariate RWA extension (LeBreton & Tonidandel, 2008) can be used to determine overall importance in explaining the multivariate criterion space. This is more complex, but it basically combines the idea of canonical correlation with relative importance. This could be useful, for instance, in educational research where you care about predicting a bundle of outcomes (grades in math _and_ science, for example) from some predictors; you'd get a sense of which predictors are globally most important to the set of outcomes. - **Interactions or Nonlinear Terms:** If your model includes interaction terms or polynomial terms (which themselves are often highly correlated with the main effects), you have to be careful because importance analysis methods assume predictors are not an arbitrary set but include all necessary components. Tonidandel & LeBreton (2009) provided guidance on handling models with interactions – essentially suggesting a procedure where higher-order terms are residualized with respect to lower-order terms to maintain interpretability. If you are examining relative importance in such models, consult their work; it's a more advanced scenario. In many cases, one might enter interaction terms and simply use RWA, but due to the hierarchy, the interpretation of weights for main effect vs interaction needs care (since the interaction's importance is conditional on being allowed to explain something above and beyond the main effects). This is a niche consideration but worth noting if your analysis goes in that direction."},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"presenting-and-reporting-results","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"7. Presenting and Reporting Results:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"reporting RWA results, ’s best transparent thorough. Report model’s R² (readers know total variance explained) R² partitioned among predictors. can present table predictors, raw relative weight, rescaled weight (% R²). ’s also useful report standard errors confidence intervals weights , especially want make claims significance (“predictor ’s weight significantly greater zero” “significantly important B”). space permits, might include RWA results traditional regression coefficients reference, give full picture. Often tell complementary stories: instance, variable might low beta (due suppression) high relative weight – juxtaposition finding discuss. Make sure explain prose weights mean (“variable accounted X% explainable variance Y”). Using visuals can help: bar chart rescaled weights intuitive, pie chart R² division (though pie charts can less precise – bar column chart preferred). Always cite source method (e.g., “using relative weight analysis approach Johnson (2000), recommended Tonidandel LeBreton (2011)”). signals readers known method used, can refer papers methodology.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"be-mindful-of-audience","dir":"Articles","previous_headings":"Evaluation of the Tonidandel & LeBreton Relative Weights Analysis Method > Considerations and Best Practices for Using RWA","what":"8. Be Mindful of Audience:","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Finally, consider consume results. ’s academic audience, may familiar beta coefficients relative weights. cases, might spend sentences educating reader RWA. Tonidandel & LeBreton (2011) great reference cite rationale, can briefly say “computed relative importance weights mitigate multicollinearity issues; represent predictor’s proportional contribution R².” ’s business audience general audience, may actually find relative weights intuitive (“X contributes 25% prediction”) regression coefficients (“one-unit change X changes Y …”). , tailoring explanation key. Just avoid jargon like “Johnson’s epsilon” (sometimes weights denoted epsilon\\_i literature) communicating – translate plain language.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"conclusion, Tonidandel LeBreton’s relative weights analysis method remains valuable valid approach assessing predictor importance presence multicollinearity. offers balanced way look predictive power variables accounting intercorrelations confound metrics. RWA generally superior naive methods (standardized betas, stepwise selection, etc.) situations faithfully represents variable’s contribution. performs similarly rigorous methods like dominance analysis, feasible use extending readily additional analyses (significance testing, model types). chief limitations automatically resolve issues redundant predictors small sample sizes – require researcher input cautious interpretation. following best practices – verifying model assumptions, using bootstrap confidence intervals, interpreting results context – user can effectively harness RWA gain insights variables truly drive outcomes extent. many research business analytics scenarios today, RWA provides clear, quantifiable answer question, “factors matter , much contribute?” – answer often obscured using standard regression collinear world illuminated relative weights method. considerations outlined , one can confidently apply relative weights analysis communicate findings guide decisions, theory development, model refinement, making valuable component analytical toolkit 2025 beyond.","code":""},{"path":"https://martinctc.github.io/rwa/articles/evaluating-rwa-method-reference.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Evaluating the Tonidandel & LeBreton Relative Weights Analysis Method","text":"Relative Importance Analysis: Useful Supplement Regression Analysis JSTOR Stadler, M., Cooper-Thomas, H. D., & Greiff, S. (2017). primer relative importance analysis: Illustrations utility psychological research. Psychological Test Assessment Modeling, 59(4), 381–403. Introduction Relative Weights Analysis rwa package Stadler, M., Cooper-Thomas, H. D., & Greiff, S. (2017). primer relative importance analysis: Illustrations utility psychological research. Psychological Test Assessment Modeling, 59(4), 381–403. 4 reasons compute importance using Relative Weights rather Shapley Regression | R-bloggers Relative Importance RWA Web — Scott Tonidandel, Ph.D.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Relative Weights Analysis (RWA) powerful method determining relative importance predictor variables multiple regression models, particularly dealing multicollinearity. vignette provides comprehensive guide using rwa package, explaining methodology, interpreting results, exploring advanced features, evaluating theoretical foundations validity method.","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"what-is-relative-weights-analysis","dir":"Articles","previous_headings":"Background and Methodology","what":"What is Relative Weights Analysis?","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Relative Weights Analysis addresses fundamental problem multiple regression: predictor variables correlated (multicollinearity), becomes difficult determine true contribution variable outcome. Traditional regression coefficients can misleading, unstable, difficult interpret presence multicollinearity. RWA decomposes total variance predicted regression model (R²) weights accurately reflect proportional contribution various predictor variables. method implemented package based Tonidandel LeBreton (2015), original roots Johnson (2000).","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"how-rwa-works","dir":"Articles","previous_headings":"Background and Methodology","what":"How RWA Works","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"core insight RWA create set orthogonal (uncorrelated) variables maximally related original predictors, use regression model. process involves: Eigenvalue decomposition predictor correlation matrix Transformation predictors orthogonal components Regression using transformed predictors Decomposition R² relative weights approach allows us determine variable’s unique contribution explained variance, even predictors highly correlated.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"when-to-use-rwa","dir":"Articles","previous_headings":"Background and Methodology","what":"When to Use RWA","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA particularly useful : Multicollinearity present among predictors need rank variables importance Traditional regression coefficients unstable difficult interpret want understand proportional contributions explained variance Conducting key drivers analysis market research business analytics","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"core-methodological-insight","dir":"Articles","previous_headings":"Theoretical Foundation and Methodological Evaluation","what":"Core Methodological Insight","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA addresses multicollinearity attributing shared (collinear) variance predictors predictors proportion structure outcome. Unlike traditional regression coefficients reflect unique contributions, RWA considers direct indirect (shared) effects. Johnson (2000) algorithm performs eigenvalue decomposition predictor correlation matrix create uncorrelated principal components, regresses outcome components. procedure yields weights virtually identical obtained averaging predictor’s incremental R² contribution possible subsets predictors (logic used dominance analysis), without brute-force search subsets, making computationally efficient.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"demonstrating-rwas-theoretical-properties","dir":"Articles","previous_headings":"Theoretical Foundation and Methodological Evaluation","what":"Demonstrating RWA’s Theoretical Properties","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Let’s demonstrate controlled example know true relationships:","code":"# Create controlled scenario to demonstrate RWA's theoretical properties set.seed(123) n <- 200  # Generate predictors with known correlation structure x1 <- rnorm(n) x2 <- 0.7 * x1 + 0.3 * rnorm(n)  # r ≈ 0.7 with x1 x3 <- 0.5 * x1 + 0.8 * rnorm(n)  # r ≈ 0.5 with x1 x4 <- rnorm(n)                    # Independent  # True population model with known coefficients y <- 0.6 * x1 + 0.4 * x2 + 0.3 * x3 + 0.2 * x4 + rnorm(n, sd = 0.5)  theory_data <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)  # Compare traditional regression vs RWA lm_theory <- lm(y ~ x1 + x2 + x3 + x4, data = theory_data) rwa_theory <- rwa(theory_data, \"y\", c(\"x1\", \"x2\", \"x3\", \"x4\"))  # Show how multicollinearity affects traditional coefficients cat(\"True population contributions (designed into simulation):\\n\") #> True population contributions (designed into simulation): true_contributions <- c(0.6, 0.4, 0.3, 0.2) names(true_contributions) <- c(\"x1\", \"x2\", \"x3\", \"x4\") print(true_contributions) #>  x1  x2  x3  x4  #> 0.6 0.4 0.3 0.2  cat(\"\\nStandardized regression coefficients (distorted by multicollinearity):\\n\") #>  #> Standardized regression coefficients (distorted by multicollinearity): std_betas <- summary(lm_theory)$coefficients[2:5, \"Estimate\"] names(std_betas) <- c(\"x1\", \"x2\", \"x3\", \"x4\") print(round(std_betas, 3)) #>    x1    x2    x3    x4  #> 0.449 0.586 0.285 0.164  cat(\"\\nRWA weights (better reflect true importance despite correlations):\\n\") #>  #> RWA weights (better reflect true importance despite correlations): rwa_weights_theory <- rwa_theory$result$Raw.RelWeight names(rwa_weights_theory) <- rwa_theory$result$Predictors print(round(rwa_weights_theory, 3)) #> [1] 0.306 0.304 0.155 0.021  # Calculate correlation between methods and true values cor_with_true <- data.frame(   Method = c(\"Std_Betas\", \"RWA_Weights\"),   Correlation_with_True = c(     cor(abs(std_betas), true_contributions),     cor(rwa_weights_theory[names(true_contributions)], true_contributions)   ) ) print(\"Correlation with true population values:\") #> [1] \"Correlation with true population values:\" print(cor_with_true) #>        Method Correlation_with_True #> 1   Std_Betas             0.6924877 #> 2 RWA_Weights                    NA"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"current-validity-why-rwa-remains-relevant","dir":"Articles","previous_headings":"Theoretical Foundation and Methodological Evaluation","what":"Current Validity: Why RWA Remains Relevant","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA remains widely accepted useful method evaluating predictor importance multicollinearity. Since introduction, numerous studies validated RWA provides fair assessment variable’s importance even high-correlation settings. Evidence research practice: - Organizational Psychology: Determining employee factors predict performance intercorrelated - Marketing Research: Key driver analysis product attributes tend move together - Educational Research: Assessing cognitive abilities naturally correlate - Healthcare Analytics: Understanding risk factors often co-occur Methodological validation: 1. RWA reveals important predictors regression beta weights obscure due multicollinearity 2. Rankings RWA correlate > 0.99 dominance analysis scenarios 3. Bootstrap confidence intervals provide robust significance testing 4. Computational efficiency allows analysis large predictor sets","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"simple-example-with-mtcars","dir":"Articles","previous_headings":"Basic Usage","what":"Simple Example with mtcars","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Let’s start basic example using built-mtcars dataset predict fuel efficiency (mpg):","code":"# Basic RWA result_basic <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"))  # View the results result_basic$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1        hp     0.2321744           29.79691    - #> 2       cyl     0.2284797           29.32274    - #> 3      disp     0.2221469           28.50999    - #> 4      gear     0.0963886           12.37037    +"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"understanding-the-output","dir":"Articles","previous_headings":"Basic Usage","what":"Understanding the Output","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"basic output includes several key components:","code":"# Predictor variables used result_basic$predictors #> [1] \"cyl\"  \"disp\" \"hp\"   \"gear\"  # Model R-squared result_basic$rsquare #> [1] 0.7791896  # Number of complete observations result_basic$n #> [1] 32  # Correlation matrices (for advanced users) str(result_basic$RXX)  # Predictor correlation matrix #>  num [1:4, 1:4] 1 0.902 0.832 -0.493 0.902 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : chr [1:4] \"cyl\" \"disp\" \"hp\" \"gear\" #>   ..$ : chr [1:4] \"cyl\" \"disp\" \"hp\" \"gear\" str(result_basic$RXY)  # Predictor-outcome correlations #>  Named num [1:4] -0.852 -0.848 -0.776 0.48 #>  - attr(*, \"names\")= chr [1:4] \"cyl\" \"disp\" \"hp\" \"gear\""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"interpreting-the-results-table","dir":"Articles","previous_headings":"Basic Usage","what":"Interpreting the Results Table","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"main results table contains: Variables: Names predictor variables Raw.RelWeight: Raw relative weights (sum R²) Rescaled.RelWeight: Rescaled weights percentages (sum 100%) Sign: Direction relationship outcome (+/-) default, results automatically sorted rescaled relative weights descending order, making easy identify important predictors:","code":"# Results are sorted by default (most important first) result_basic$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1        hp     0.2321744           29.79691    - #> 2       cyl     0.2284797           29.32274    - #> 3      disp     0.2221469           28.50999    - #> 4      gear     0.0963886           12.37037    +  # Raw weights sum to R-squared sum(result_basic$result$Raw.RelWeight) #> [1] 0.7791896 result_basic$rsquare #> [1] 0.7791896  # Rescaled weights sum to 100% sum(result_basic$result$Rescaled.RelWeight) #> [1] 100"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"controlling-output-sorting","dir":"Articles","previous_headings":"Basic Usage","what":"Controlling Output Sorting","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"can control whether results sorted using sort parameter:","code":"# Default behavior: sorted by importance (descending) result_sorted <- mtcars %>%   rwa(outcome = \"mpg\", predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"))  result_sorted$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1        hp     0.2321744           29.79691    - #> 2       cyl     0.2284797           29.32274    - #> 3      disp     0.2221469           28.50999    - #> 4      gear     0.0963886           12.37037    +  # Preserve original predictor order result_unsorted <- mtcars %>%   rwa(outcome = \"mpg\", predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"), sort = FALSE)  result_unsorted$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1       cyl     0.2284797           29.32274    - #> 2      disp     0.2221469           28.50999    - #> 3        hp     0.2321744           29.79691    - #> 4      gear     0.0963886           12.37037    +"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"adding-signs-to-weights","dir":"Articles","previous_headings":"Advanced Features","what":"Adding Signs to Weights","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"applysigns parameter adds directional information help interpret whether variables positively negatively influence outcome:","code":"result_signs <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"),       applysigns = TRUE)  result_signs$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Sign.Rescaled.RelWeight #> 1        hp     0.2321744           29.79691    -               -29.79691 #> 2       cyl     0.2284797           29.32274    -               -29.32274 #> 3      disp     0.2221469           28.50999    -               -28.50999 #> 4      gear     0.0963886           12.37037    +                12.37037"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"visualization","dir":"Articles","previous_headings":"Advanced Features","what":"Visualization","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"package allows visualize relative importance piping results plot_rwa():","code":"# Generate RWA results  rwa_result <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\", \"wt\"))  # Create plot rwa_result %>% plot_rwa() # The rescaled relative weights rwa_result$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1        wt    0.23642417          27.778646    - #> 2       cyl    0.18833374          22.128264    - #> 3        hp    0.18809142          22.099792    - #> 4      disp    0.16479802          19.362936    - #> 5      gear    0.07345304           8.630361    +"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"bootstrap-confidence-intervals","dir":"Articles","previous_headings":"","what":"Bootstrap Confidence Intervals","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"rwa package includes powerful bootstrap functionality determining statistical significance relative weights. detailed coverage bootstrap methods, see dedicated vignette:","code":"vignette(\"bootstrap-confidence-intervals\", package = \"rwa\")"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"quick-bootstrap-example","dir":"Articles","previous_headings":"Bootstrap Confidence Intervals","what":"Quick Bootstrap Example","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"comprehensive coverage bootstrap methods, advanced features, best practices, consult bootstrap vignette.","code":"# Basic bootstrap analysis bootstrap_result <- mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\"),       bootstrap = TRUE,       n_bootstrap = 500)  # Reduced for speed  # View significant predictors bootstrap_result$result %>%   filter(Raw.Significant == TRUE) %>%   select(Variables, Rescaled.RelWeight, Raw.RelWeight.CI.Lower, Raw.RelWeight.CI.Upper) #>   Variables Rescaled.RelWeight Raw.RelWeight.CI.Lower Raw.RelWeight.CI.Upper #> 1      disp           36.37966              0.2019181              0.3411714 #> 2       cyl           35.46279              0.2175689              0.3301762 #> 3        hp           28.15755              0.1566494              0.2638215"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"real-world-example-diamond-price-analysis","dir":"Articles","previous_headings":"","what":"Real-World Example: Diamond Price Analysis","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Let’s explore complex example using diamonds dataset: bootstrap analysis example confidence intervals, see:","code":"# Analyze diamond price drivers diamonds_subset <- diamonds %>%   select(price, carat, depth, table, x, y, z) %>%   sample_n(1000)  # Sample for faster computation  diamond_rwa <- diamonds_subset %>%   rwa(outcome = \"price\",       predictors = c(\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"),       applysigns = TRUE)  diamond_rwa$result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Sign.Rescaled.RelWeight #> 1     carat   0.253554640         28.9515921    +              28.9515921 #> 2         y   0.206371378         23.5640727    +              23.5640727 #> 3         z   0.204433608         23.3428125    +              23.3428125 #> 4         x   0.204002285         23.2935629    +              23.2935629 #> 5     table   0.004842128          0.5528879    +               0.5528879 #> 6     depth   0.002584205          0.2950719    -              -0.2950719 vignette(\"bootstrap-confidence-intervals\", package = \"rwa\")"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"comparison-with-traditional-regression","dir":"Articles","previous_headings":"","what":"Comparison with Traditional Regression","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Let’s compare RWA results traditional multiple regression highlight differences:","code":"# Traditional regression lm_model <- lm(mpg ~ cyl + disp + hp + gear, data = mtcars) lm_summary <- summary(lm_model)  # Display regression summary print(lm_summary) #>  #> Call: #> lm(formula = mpg ~ cyl + disp + hp + gear, data = mtcars) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -4.3716 -2.3319 -0.8279  1.3156  7.0782  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 27.42342    6.30108   4.352 0.000173 *** #> cyl         -0.86624    0.84941  -1.020 0.316869     #> disp        -0.01190    0.01190  -1.000 0.325996     #> hp          -0.03050    0.01982  -1.539 0.135498     #> gear         1.42306    1.21053   1.176 0.250030     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 3.035 on 27 degrees of freedom #> Multiple R-squared:  0.7792, Adjusted R-squared:  0.7465  #> F-statistic: 23.82 on 4 and 27 DF,  p-value: 1.606e-08  # RWA results rwa_model <- mtcars %>%   rwa(outcome = \"mpg\", predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"))  # Compare importance rankings comparison <- data.frame(   Variable = rwa_model$predictors,   RWA_Rescaled = rwa_model$result$Rescaled.RelWeight,   RWA_Rank = rank(-rwa_model$result$Rescaled.RelWeight) )  print(comparison) #>   Variable RWA_Rescaled RWA_Rank #> 1      cyl     29.79691        1 #> 2     disp     29.32274        2 #> 3       hp     28.50999        3 #> 4     gear     12.37037        4"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"superior-to-standard-regression-under-multicollinearity","dir":"Articles","previous_headings":"Comparison with Traditional Regression > Methodological Advantages of RWA","what":"1. Superior to Standard Regression Under Multicollinearity","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Traditional standardized beta coefficients reflect unique contribution holding others constant, severely underestimates variables share variance others. RWA gives credit predictors variance share outcome individually jointly variables.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"avoids-stepwise-selection-problems","dir":"Articles","previous_headings":"Comparison with Traditional Regression > Methodological Advantages of RWA","what":"2. Avoids Stepwise Selection Problems","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Stepwise methods arbitrarily pick one variable collinear set drop others, potentially leading incorrect conclusions importance. Unlike stepwise selection, RWA throw away information – shows much predictor contributes given complete set.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"more-interpretable-than-principal-components","dir":"Articles","previous_headings":"Comparison with Traditional Regression > Methodological Advantages of RWA","what":"3. More Interpretable than Principal Components","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"PCA addresses multicollinearity, buries identity individual predictors composite factors. RWA uses orthogonalization behind scenes returns results terms original predictors, maintaining interpretability.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"computational-efficiency-vs--dominance-analysis","dir":"Articles","previous_headings":"Comparison with Traditional Regression > Methodological Advantages of RWA","what":"4. Computational Efficiency vs. Dominance Analysis","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA’s greatest achievement producing almost identical results dominance analysis far efficiently. Research shows: Correlation > 0.99 RWA dominance analysis rankings Exponential time savings: dominance analysis requires 2^p models; RWA solves equations directly Enhanced capabilities: easier provide confidence intervals significance tests","code":"# Demonstrate computational considerations predictors <- c(\"cyl\", \"disp\", \"hp\", \"gear\") n_predictors <- length(predictors)  cat(\"Number of predictors:\", n_predictors, \"\\n\") #> Number of predictors: 4 cat(\"Dominance analysis would require\", 2^n_predictors, \"subset models\\n\") #> Dominance analysis would require 16 subset models cat(\"RWA solves this in a single matrix operation\\n\") #> RWA solves this in a single matrix operation  # Show RWA speed (for demonstration) start_time <- Sys.time() rwa_speed_test <- mtcars %>% rwa(outcome = \"mpg\", predictors = predictors) end_time <- Sys.time() cat(\"RWA computation time:\", round(as.numeric(end_time - start_time, units = \"secs\"), 4), \"seconds\\n\") #> RWA computation time: 0.0084 seconds"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"not-a-replacement-for-theory","dir":"Articles","previous_headings":"Critical Limitations and When to Exercise Caution","what":"1. Not a Replacement for Theory","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA fundamentally descriptive, variance-partitioning tool. tells us “much prediction attributable X” imply causation. Tonidandel & LeBreton emphasize: relative weights “causal indicators thus necessarily dictate course action.” RWA enhance interpretation regression models decisions must still guided substantive theory.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"the-redundancy-problem","dir":"Articles","previous_headings":"Critical Limitations and When to Exercise Caution","what":"2. The Redundancy Problem","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA magic bullet extreme multicollinearity stemming redundant predictors. predictors measure essentially construct, RWA split contribution , potentially making appear less important individually. statistically appropriate – reflects one variable alone job . Users must recognize pair whole may important even alone modest weight.","code":"# Demonstrate the redundancy limitation set.seed(456) x1_orig <- rnorm(100) x1_dup <- x1_orig + rnorm(100, sd = 0.05)  # Nearly identical (r ≈ 0.99) y_simple <- 0.8 * x1_orig + rnorm(100, sd = 0.5)  redundant_data <- data.frame(y = y_simple, x1_original = x1_orig,                             x1_duplicate = x1_dup)  cat(\"Correlation between 'different' predictors:\", cor(x1_orig, x1_dup), \"\\n\") #> Correlation between 'different' predictors: 0.9988244  # RWA correctly splits redundant variance redundant_rwa <- rwa(redundant_data, \"y\", c(\"x1_original\", \"x1_duplicate\")) print(\"RWA with redundant predictors:\") #> [1] \"RWA with redundant predictors:\" print(redundant_rwa$result) #>      Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1  x1_original     0.3841750           50.05342    + #> 2 x1_duplicate     0.3833551           49.94658    +  cat(\"\\nEach variable appears less important individually,\") #>  #> Each variable appears less important individually, cat(\"\\nbut together they account for most variance.\\n\") #>  #> but together they account for most variance. cat(\"Combined contribution:\",      sum(redundant_rwa$result$Raw.RelWeight), \"\\n\") #> Combined contribution: 0.7675301"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"sample-size-and-stability","dir":"Articles","previous_headings":"Critical Limitations and When to Exercise Caution","what":"3. Sample Size and Stability","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA shares multiple regression’s sensitivity sample size predictor--observation ratios. Bootstrap confidence intervals help quantify uncertainty, “bootstrapping overcome inherent limitations associated small sample size.”","code":""},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"sample-size-considerations","dir":"Articles","previous_headings":"Best Practices and Recommendations","what":"1. Sample Size Considerations","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"","code":"# Check your sample size n_obs <- mtcars %>%    select(mpg, cyl, disp, hp, gear) %>%    na.omit() %>%    nrow()  cat(\"Sample size:\", n_obs) #> Sample size: 32 cat(\"\\nRule of thumb: At least 5-10 observations per predictor\") #>  #> Rule of thumb: At least 5-10 observations per predictor"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"variable-selection-and-model-specification","dir":"Articles","previous_headings":"Best Practices and Recommendations","what":"2. Variable Selection and Model Specification","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA works best : Continuous variables (though categorical can used caution) Theoretically meaningful predictors Reasonable predictor--observation ratios Predictors distinct may correlated","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"when-to-use-rwa-vs--alternatives","dir":"Articles","previous_headings":"Best Practices and Recommendations","what":"3. When to Use RWA vs. Alternatives","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Use RWA : Predictors theoretically distinct empirically correlated Understanding relative contribution (just prediction) goal Sample size supports stable multiple regression Linear relationships reasonable assumptions Consider alternatives : Predictors essentially redundant measures Sample size small relative predictors Primary goal variable selection rather interpretation Non-linear effects suspected","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"reporting-results","dir":"Articles","previous_headings":"Best Practices and Recommendations","what":"4. Reporting Results","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"reporting RWA results, include: Sample size missing data handling Raw rescaled weights Model R-squared Confidence intervals key predictors (using bootstrap) bootstrap confidence intervals advanced statistical considerations, see:","code":"vignette(\"bootstrap-confidence-intervals\", package = \"rwa\")"},{"path":[]},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"multicollinearity-warnings","dir":"Articles","previous_headings":"Troubleshooting Common Issues","what":"Multicollinearity Warnings","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"encounter extreme multicollinearity:","code":"# Check correlation matrix cor_matrix <- mtcars %>%   select(cyl, disp, hp, gear) %>%   cor()  # Look for high correlations (>0.9) high_cor <- which(abs(cor_matrix) > 0.9 & cor_matrix != 1, arr.ind = TRUE) if(nrow(high_cor) > 0) {   cat(\"High correlations detected between variables\") } #> High correlations detected between variables"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"missing-data","dir":"Articles","previous_headings":"Troubleshooting Common Issues","what":"Missing Data","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA handles missing data listwise deletion:","code":"# Check for missing data patterns missing_summary <- mtcars %>%   select(mpg, cyl, disp, hp, gear) %>%   summarise_all(~sum(is.na(.)))  print(missing_summary) #>   mpg cyl disp hp gear #> 1   0   0    0  0    0"},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"package methodology based following key references: Primary Sources: Johnson, J. W. (2000). heuristic method estimating relative weight predictor variables multiple regression. Multivariate Behavioral Research, 35(1), 1-19. DOI: 10.1207/S15327906MBR3501_1 Tonidandel, S., & LeBreton, J. M. (2015). RWA Web: free, comprehensive, web-based, user-friendly tool relative weight analyses. Journal Business Psychology, 30(2), 207-216. DOI: 10.1007/s10869-014-9351-z Additional Reading: Azen, R., & Budescu, D. V. (2003). dominance analysis approach comparing predictors multiple regression. Psychological Methods, 8(2), 129-148. Grömping, U. (2006). Relative importance linear regression R: package relaimpo. Journal Statistical Software, 17(1), 1-27. Johnson, J. W., & LeBreton, J. M. (2004). History use relative importance indices organizational research. Organizational Research Methods, 7(3), 238-257. bootstrap methods statistical significance testing, see dedicated bootstrap vignette references. latest methodological developments: https://www.scotttonidandel.com/rwa-web/","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"rwa package provides user-friendly implementation Relative Weights Analysis R seamless tidyverse integration. vignette covered practical implementation theoretical foundations.","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"key-takeaways","dir":"Articles","previous_headings":"Conclusion","what":"Key Takeaways","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"Methodological Strengths: Handles multicollinearity effectively partitioning shared variance appropriately Provides interpretable results percentages explained variance Computationally efficient compared dominance analysis Statistically robust bootstrap confidence intervals Important Limitations: replacement theory - results descriptive, causal Redundant predictors split variance, potentially appearing less important individually Sample size matters - adequate observations needed stable estimates Linear model assumptions apply","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"when-to-use-rwa-1","dir":"Articles","previous_headings":"Conclusion","what":"When to Use RWA","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA particularly valuable : Key drivers analysis market research business analytics Multicollinear predictors scientific research Situations requiring interpretable importance measures Linear models traditional regression coefficients misleading","code":""},{"path":"https://martinctc.github.io/rwa/articles/introduction-to-rwa.html","id":"the-bottom-line","dir":"Articles","previous_headings":"Conclusion","what":"The Bottom Line","title":"Introduction to Relative Weights Analysis with the rwa Package","text":"RWA provides clear, quantifiable answers questions predictor importance multicollinear world – answers often obscured using standard regression approaches. proper understanding assumptions limitations, RWA remains essential tool researchers practitioners dealing correlated predictors. Whether ’re conducting key drivers analysis market research, exploring variable importance predictive modeling, addressing multicollinearity academic research, RWA provides valuable insights complement traditional regression approaches. advanced bootstrap methods statistical significance testing, see:","code":"vignette(\"bootstrap-confidence-intervals\", package = \"rwa\")"},{"path":"https://martinctc.github.io/rwa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martin Chan. Author, maintainer.","code":""},{"path":"https://martinctc.github.io/rwa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chan M (2025). rwa: Perform Relative Weights Analysis. R package version 0.1.0, https://martinctc.github.io/rwa/.","code":"@Manual{,   title = {rwa: Perform a Relative Weights Analysis},   author = {Martin Chan},   year = {2025},   note = {R package version 0.1.0},   url = {https://martinctc.github.io/rwa/}, }"},{"path":"https://martinctc.github.io/rwa/index.html","id":"rwa","dir":"","previous_headings":"","what":"Perform a Relative Weights Analysis","title":"Perform a Relative Weights Analysis","text":"Perform Relative Weights Analysis R","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"background","dir":"","previous_headings":"","what":"Background","title":"Perform a Relative Weights Analysis","text":"Relative Weights Analysis (RWA) method calculating relative importance predictor variables contributing outcome variable. method implemented function based Tonidandel LeBreton (2015), origin specific approach can traced back Johnson (2000), Heuristic Method Estimating Relative Weight Predictor Variables Multiple Regression. Broadly speaking, RWA belongs family techiques broad umbrella ‘Relative Importance Analysis’, members include ‘Shapley method’ ‘dominance analysis’. often referred ‘Key Drivers Analysis’ within market research. package built around main function rwa(), takes data frame argument allows specify names input variables outcome variable arguments. rwa() function package compatible dplyr / tidyverse style piping operations enable cleaner readable code.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Perform a Relative Weights Analysis","text":"can install stable CRAN version rwa : Alternatively, can install latest development version GitHub :","code":"install.packages(\"rwa\") install.packages(\"devtools\") devtools::install_github(\"martinctc/rwa\")"},{"path":"https://martinctc.github.io/rwa/index.html","id":"method--technical-details","dir":"","previous_headings":"","what":"Method / Technical details","title":"Perform a Relative Weights Analysis","text":"RWA decomposes total variance predicted regression model (R2) weights accurately reﬂect proportional contribution various predictor variables. RWA useful technique calculate relative importance predictors (independent variables) independent variables correlated . alternative multiple regression technique addresses multicollinearity problem, also helps calculate importance rank variables. helps answer “variable important rank variables based contribution R-Square”. See https://link.springer.com/content/pdf/10.1007%2Fs10869-014-9351-z.pdf.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"multicollinearity","dir":"","previous_headings":"","what":"Multicollinearity","title":"Perform a Relative Weights Analysis","text":"independent variables correlated, difficult determine correct prediction power variable. Hence, difficult rank unable estimate coefficients correctly. Statistically, multicollinearity can increase standard error coefficient estimates make estimates sensitive minor changes model. means coefficients biased difficult interpret.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"signs","dir":"","previous_headings":"","what":"Signs","title":"Perform a Relative Weights Analysis","text":"Key Drivers Analysis methods conventionally include score sign, can make difficult interpret whether variable positively negatively driving outcome. applysigns argument rwa::rwa(), set TRUE, allows application positive negative signs driver scores match signs corresponding linear regression coefficients model. feature mimics solution used Q research software. resulting column labelled Sign.Rescaled.RelWeight distinguish unsigned column.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"estimating-the-statistical-significance-of-relative-weights","dir":"","previous_headings":"","what":"Estimating the statistical significance of relative weights","title":"Perform a Relative Weights Analysis","text":"Tonidandel et al. (2009) noted, default procedure determining statistical significance individual relative weights: difficulty determining statistical significance relative weights stems fact exact (small sample) sampling distribution relative weights unknown. paper suggests Monte Carlo method estimating statistical significance, currently available provided package, plan implement near future.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic example","title":"Perform a Relative Weights Analysis","text":"can pass raw data directly rwa(), without first compute correlation matrix. example mtcars. Code: Results:","code":"library(rwa) library(tidyverse)  mtcars %>%   rwa(outcome = \"mpg\",       predictors = c(\"cyl\", \"disp\", \"hp\", \"gear\"),       applysigns = TRUE) $predictors [1] \"cyl\"  \"disp\" \"hp\"   \"gear\"  $rsquare [1] 0.7791896  $result   Variables Raw.RelWeight Rescaled.RelWeight Sign Sign.Rescaled.RelWeight 1       cyl     0.2284797           29.32274    -               -29.32274 2      disp     0.2221469           28.50999    -               -28.50999 3        hp     0.2321744           29.79691    -               -29.79691 4      gear     0.0963886           12.37037    +                12.37037  $n [1] 32"},{"path":"https://martinctc.github.io/rwa/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Perform a Relative Weights Analysis","text":"comprehensive examples methodology explanations, see package vignettes: vignettes cover: Introduction vignette: Detailed methodology, basic examples, use RWA Bootstrap vignette: Statistical significance testing, confidence intervals, advanced features Real-world applications best practices Troubleshooting interpretation guidance","code":"# Main introduction to RWA vignette(\"introduction-to-rwa\", package = \"rwa\")  # Bootstrap confidence intervals for statistical significance vignette(\"bootstrap-confidence-intervals\", package = \"rwa\")"},{"path":"https://martinctc.github.io/rwa/index.html","id":"latest-status","dir":"","previous_headings":"","what":"Latest Status","title":"Perform a Relative Weights Analysis","text":"main rwa() function ready--use now includes bootstrap confidence intervals determining statistical significance relative weights. package includes comprehensive documentation vignettes designed integrate seamlessly tidyverse workflows.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"contact-me","dir":"","previous_headings":"","what":"Contact me","title":"Perform a Relative Weights Analysis","text":"Please feel free submit suggestions report bugs: https://github.com/martinctc/rwa/issues Also check website work packages.","code":""},{"path":"https://martinctc.github.io/rwa/index.html","id":"references--bibliography","dir":"","previous_headings":"","what":"References / Bibliography","title":"Perform a Relative Weights Analysis","text":"Azen, R., & Budescu, D. V. (2003). dominance analysis approach comparing predictors multiple regression. Psychological methods, 8(2), 129. Budescu, D. V. (1993). Dominance analysis: new approach problem relative importance predictors multiple regression. Psychological bulletin, 114(3), 542. Grömping, U. (2006). Relative importance linear regression R: package relaimpo. Journal statistical software, 17(1), 1-27. Grömping, U. (2009). Variable importance assessment regression: linear regression versus random forest. American Statistician, 63(4), 308-319. Johnson, J. W., & LeBreton, J. M. (2004). History use relative importance indices organizational research. Organizational research methods, 7(3), 238-257. Lindeman RH, Merenda PF, Gold RZ (1980). Introduction Bivariate Multivariate Analysis. Scott, Foresman, Glenview, IL. Tonidandel, S., & LeBreton, J. M. (2011). Relative importance analysis: useful supplement regression analysis. Journal Business Psychology, 26(1), 1-9. Tonidandel, S., LeBreton, J. M., & Johnson, J. W. (2009). Determining statistical significance relative weights. Psychological methods, 14(4), 387. Wang, X., Duverger, P., Bansal, H. S. (2013). Bayesian Inference Predictors Relative Importance Linear Regression Model using Dominance Hierarchies. International Journal Pure Applied Mathematics, Vol. 88, . 3, 321-339. Also see Kovalyshyn similar implementation Javascript.","code":""},{"path":"https://martinctc.github.io/rwa/reference/extract_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract confidence intervals from bootstrap object — extract_ci","title":"Extract confidence intervals from bootstrap object — extract_ci","text":"Extract confidence intervals bootstrap object","code":""},{"path":"https://martinctc.github.io/rwa/reference/extract_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract confidence intervals from bootstrap object — extract_ci","text":"","code":"extract_ci(   boot_object,   conf_level = 0.95,   variable_names = NULL,   ci_type = \"raw\" )"},{"path":"https://martinctc.github.io/rwa/reference/extract_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract confidence intervals from bootstrap object — extract_ci","text":"boot_object Boot object boot::boot() conf_level Confidence level (default 0.95) variable_names Names variables labeling ci_type Type CI extract (\"raw\", \"rand_diff\", \"focal_diff\")","code":""},{"path":"https://martinctc.github.io/rwa/reference/extract_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract confidence intervals from bootstrap object — extract_ci","text":"Data frame confidence intervals","code":""},{"path":"https://martinctc.github.io/rwa/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://martinctc.github.io/rwa/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://martinctc.github.io/rwa/reference/plot_rwa.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the rescaled importance values from the output of rwa() — plot_rwa","title":"Plot the rescaled importance values from the output of rwa() — plot_rwa","text":"Pass output rwa() plot bar chart rescaled importance values. Signs always calculated taken account, equivalent setting applysigns argument TRUE rwa().","code":""},{"path":"https://martinctc.github.io/rwa/reference/plot_rwa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the rescaled importance values from the output of rwa() — plot_rwa","text":"","code":"plot_rwa(rwa)"},{"path":"https://martinctc.github.io/rwa/reference/plot_rwa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the rescaled importance values from the output of rwa() — plot_rwa","text":"rwa Direct list output rwa().","code":""},{"path":"https://martinctc.github.io/rwa/reference/plot_rwa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the rescaled importance values from the output of rwa() — plot_rwa","text":"","code":"library(ggplot2) # Use a smaller sample for faster execution diamonds_small <- diamonds[sample(nrow(diamonds), 1000), ] diamonds_small %>%   rwa(outcome = \"price\",       predictors = c(\"depth\",\"carat\", \"x\", \"y\", \"z\"),       applysigns = TRUE) %>%   plot_rwa()"},{"path":"https://martinctc.github.io/rwa/reference/remove_all_na_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove any columns where all the values are missing — remove_all_na_cols","title":"Remove any columns where all the values are missing — remove_all_na_cols","text":"Pass data frame returns version columns made entirely missing values removed.","code":""},{"path":"https://martinctc.github.io/rwa/reference/remove_all_na_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove any columns where all the values are missing — remove_all_na_cols","text":"","code":"remove_all_na_cols(df)"},{"path":"https://martinctc.github.io/rwa/reference/remove_all_na_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove any columns where all the values are missing — remove_all_na_cols","text":"df Data frame passed .","code":""},{"path":"https://martinctc.github.io/rwa/reference/remove_all_na_cols.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove any columns where all the values are missing — remove_all_na_cols","text":"used within rwa().","code":""},{"path":"https://martinctc.github.io/rwa/reference/run_rwa_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Run bootstrap analysis for RWA — run_rwa_bootstrap","title":"Run bootstrap analysis for RWA — run_rwa_bootstrap","text":"Run bootstrap analysis RWA","code":""},{"path":"https://martinctc.github.io/rwa/reference/run_rwa_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run bootstrap analysis for RWA — run_rwa_bootstrap","text":"","code":"run_rwa_bootstrap(   data,   outcome,   predictors,   n_bootstrap = 1000,   conf_level = 0.95,   focal = NULL,   comprehensive = FALSE,   include_rescaled = FALSE )"},{"path":"https://martinctc.github.io/rwa/reference/run_rwa_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run bootstrap analysis for RWA — run_rwa_bootstrap","text":"data Data frame outcome Outcome variable predictors Predictor variables n_bootstrap Number bootstrap samples conf_level Confidence level focal Focal variable comparisons (optional) comprehensive Whether run comprehensive analysis include_rescaled Whether bootstrap rescaled weights","code":""},{"path":"https://martinctc.github.io/rwa/reference/run_rwa_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run bootstrap analysis for RWA — run_rwa_bootstrap","text":"List bootstrap results confidence intervals","code":""},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Relative Weights Analysis (RWA) — rwa","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"function creates Relative Weights Analysis (RWA) returns list outputs. RWA provides heuristic method estimating relative weight predictor variables multiple regression, involves creating multiple regression set transformed predictors orthogonal maximally related original set predictors. rwa() optimised dplyr pipes shows positive / negative signs weights.","code":""},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"","code":"rwa(   df,   outcome,   predictors,   applysigns = FALSE,   sort = TRUE,   bootstrap = FALSE,   n_bootstrap = 1000,   conf_level = 0.95,   focal = NULL,   comprehensive = FALSE,   include_rescaled_ci = FALSE )"},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"df Data frame tibble passed . outcome Outcome variable, specified string bare input. Must numeric variable. predictors Predictor variable(s), specified vector string(s) bare input(s). variables must numeric. applysigns Logical value specifying whether show estimate applies sign. Defaults FALSE. sort Logical value specifying whether sort results rescaled relative weights descending order. Defaults TRUE. bootstrap Logical value specifying whether calculate bootstrap confidence intervals. Defaults FALSE. n_bootstrap Number bootstrap samples use bootstrap = TRUE. Defaults 1000. conf_level Confidence level bootstrap intervals. Defaults 0.95. focal Focal variable bootstrap comparisons (optional). comprehensive Whether run comprehensive bootstrap analysis including random variable focal comparisons. include_rescaled_ci Logical value specifying whether include confidence intervals rescaled weights. Defaults FALSE due compositional data constraints. Use caution.","code":""},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"rwa() returns list outputs, follows: predictors: character vector names predictor variables used. rsquare: rsquare value regression model. result: final output importance metrics (sorted Rescaled.RelWeight descending order default). Rescaled.RelWeight column sums 100. Sign column indicates whether predictor positively negatively correlated outcome. bootstrap = TRUE, includes confidence interval columns raw weights. Rescaled weight CIs available via include_rescaled_ci = TRUE recommended inference. n: indicates number observations used analysis. bootstrap: bootstrap results (present bootstrap = TRUE), containing: ci_results: confidence intervals weights boot_object: raw bootstrap object advanced analysis n_bootstrap: number bootstrap samples used lambda: RXX: Correlation matrix predictor variables . RXY: Correlation values predictor variables outcome variable.","code":""},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"rwa() produces raw relative weight values (epsilons) well rescaled weights (scaled percentage predictable variance) every predictor model. Signs added weights applysigns argument set TRUE. See https://www.scotttonidandel.com/rwa-web original implementation inspired package.","code":""},{"path":"https://martinctc.github.io/rwa/reference/rwa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Relative Weights Analysis (RWA) — rwa","text":"","code":"library(ggplot2) # Basic RWA (results sorted by default) rwa(diamonds,\"price\",c(\"depth\",\"carat\")) #> $predictors #> [1] \"depth\" \"carat\" #>  #> $rsquare #> [1] 0.8506755 #>  #> $result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1     carat   0.849946308        99.91428588    + #> 2     depth   0.000729149         0.08571412    - #>  #> $n #> [1] 53940 #>  #> $lambda #>            [,1]       [,2] #> [1,] 0.99990040 0.01411356 #> [2,] 0.01411356 0.99990040 #>  #> $RXX #>            depth      carat #> depth 1.00000000 0.02822431 #> carat 0.02822431 1.00000000 #>  #> $RXY #>      depth      carat  #> -0.0106474  0.9215913  #>   # RWA without sorting (preserves original predictor order) rwa(diamonds,\"price\",c(\"depth\",\"carat\"), sort = FALSE) #> $predictors #> [1] \"depth\" \"carat\" #>  #> $rsquare #> [1] 0.8506755 #>  #> $result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign #> 1     depth   0.000729149         0.08571412    - #> 2     carat   0.849946308        99.91428588    + #>  #> $n #> [1] 53940 #>  #> $lambda #>            [,1]       [,2] #> [1,] 0.99990040 0.01411356 #> [2,] 0.01411356 0.99990040 #>  #> $RXX #>            depth      carat #> depth 1.00000000 0.02822431 #> carat 0.02822431 1.00000000 #>  #> $RXY #>      depth      carat  #> -0.0106474  0.9215913  #>   # \\donttest{ # For faster examples, use a subset of data for bootstrap diamonds_small <- diamonds[sample(nrow(diamonds), 1000), ]  # RWA with bootstrap confidence intervals (raw weights only) rwa(diamonds_small,\"price\",c(\"depth\",\"carat\"), bootstrap = TRUE, n_bootstrap = 100) #> Running bootstrap analysis with 100 samples... #> $predictors #> [1] \"depth\" \"carat\" #>  #> $rsquare #> [1] 0.8499131 #>  #> $result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Raw.RelWeight.CI.Lower #> 1     carat  0.8489637953         99.8883007    +            0.821172909 #> 2     depth  0.0009493473          0.1116993    -           -0.002577354 #>   Raw.RelWeight.CI.Upper Raw.Significant #> 1            0.878932350            TRUE #> 2            0.001598172           FALSE #>  #> $n #> [1] 1000 #>  #> $bootstrap #> $bootstrap$boot_object #>  #> ORDINARY NONPARAMETRIC BOOTSTRAP #>  #>  #> Call: #> boot::boot(data = bootstrap_data, statistic = rwa_boot_statistic,  #>     R = n_bootstrap, outcome = outcome, predictors = predictors) #>  #>  #> Bootstrap Statistics : #>         original        bias    std. error #> t1* 0.0009493473  0.0006083043 0.001005676 #> t2* 0.8489637953 -0.0009349687 0.014284910 #>  #> $bootstrap$ci_results #> $bootstrap$ci_results$raw_weights #> # A tibble: 2 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   #> 1 depth               1 -0.00258  0.00160 basic     raw     #> 2 carat               2  0.821    0.879   basic     raw     #>  #>  #> $bootstrap$n_bootstrap #> [1] 100 #>  #> $bootstrap$conf_level #> [1] 0.95 #>  #> $bootstrap$comprehensive #> [1] FALSE #>  #> $bootstrap$focal #> NULL #>  #>  #> $lambda #>           [,1]      [,2] #> [1,] 0.9998984 0.0142579 #> [2,] 0.0142579 0.9998984 #>  #> $RXX #>           depth     carat #> depth 1.0000000 0.0285129 #> carat 0.0285129 1.0000000 #>  #> $RXY #>       depth       carat  #> -0.01473139  0.92099482  #>   # Include rescaled weight CIs (use with caution for inference) rwa(diamonds_small,\"price\",c(\"depth\",\"carat\"), bootstrap = TRUE,      include_rescaled_ci = TRUE, n_bootstrap = 100) #> Running bootstrap analysis with 100 samples... #> Warning: Rescaled weight confidence intervals should be interpreted with caution due to compositional data constraints. Use for descriptive purposes only, not formal statistical inference. #> $predictors #> [1] \"depth\" \"carat\" #>  #> $rsquare #> [1] 0.8499131 #>  #> $result #>   Variables Raw.RelWeight Rescaled.RelWeight Sign Raw.RelWeight.CI.Lower #> 1     carat  0.8489637953         99.8883007    +            0.820116111 #> 2     depth  0.0009493473          0.1116993    -           -0.001621699 #>   Raw.RelWeight.CI.Upper Raw.Significant Rescaled.RelWeight.CI.Lower #> 1            0.874260240            TRUE                  99.8007012 #> 2            0.001587324           FALSE                  -0.2253907 #>   Rescaled.RelWeight.CI.Upper #> 1                 100.2253907 #> 2                   0.1992988 #>  #> $n #> [1] 1000 #>  #> $bootstrap #> $bootstrap$boot_object #>  #> ORDINARY NONPARAMETRIC BOOTSTRAP #>  #>  #> Call: #> boot::boot(data = bootstrap_data, statistic = rwa_boot_statistic,  #>     R = n_bootstrap, outcome = outcome, predictors = predictors) #>  #>  #> Bootstrap Statistics : #>         original       bias     std. error #> t1* 0.0009493473 0.0006033209 0.0009143211 #> t2* 0.8489637953 0.0037448894 0.0127520755 #>  #> $bootstrap$boot_object_rescaled #>  #> ORDINARY NONPARAMETRIC BOOTSTRAP #>  #>  #> Call: #> boot::boot(data = bootstrap_data, statistic = rwa_boot_statistic_rescaled,  #>     R = n_bootstrap, outcome = outcome, predictors = predictors) #>  #>  #> Bootstrap Statistics : #>       original      bias    std. error #> t1*  0.1116993  0.05818349   0.1031148 #> t2* 99.8883007 -0.05818349   0.1031148 #>  #> $bootstrap$ci_results #> $bootstrap$ci_results$raw_weights #> # A tibble: 2 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   #> 1 depth               1 -0.00162  0.00159 basic     raw     #> 2 carat               2  0.820    0.874   basic     raw     #>  #> $bootstrap$ci_results$rescaled_weights #> # A tibble: 2 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type  #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>    #> 1 depth               1   -0.225    0.199 basic     rescaled #> 2 carat               2   99.8    100.    basic     rescaled #>  #>  #> $bootstrap$n_bootstrap #> [1] 100 #>  #> $bootstrap$conf_level #> [1] 0.95 #>  #> $bootstrap$comprehensive #> [1] FALSE #>  #> $bootstrap$focal #> NULL #>  #>  #> $lambda #>           [,1]      [,2] #> [1,] 0.9998984 0.0142579 #> [2,] 0.0142579 0.9998984 #>  #> $RXX #>           depth     carat #> depth 1.0000000 0.0285129 #> carat 0.0285129 1.0000000 #>  #> $RXY #>       depth       carat  #> -0.01473139  0.92099482  #>   # Comprehensive bootstrap analysis with focal variable result <- rwa(diamonds_small,\"price\",c(\"depth\",\"carat\",\"table\"),                bootstrap = TRUE, comprehensive = TRUE, focal = \"carat\",                n_bootstrap = 100) #> Running bootstrap analysis with 100 samples... # View confidence intervals result$bootstrap$ci_results #> $raw_weights #> # A tibble: 3 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>   #> 1 depth               1 -0.00147  0.00180 basic     raw     #> 2 carat               2  0.817    0.872   basic     raw     #> 3 table               3 -0.00381  0.0148  basic     raw     #>  #> $random_comparison #> # A tibble: 3 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type   #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>     #> 1 Var4                1 -0.00134  0.00416 basic     rand_diff #> 2 Var5                2  0.816    0.870   basic     rand_diff #> 3 Var6                3 -0.00120  0.0156  basic     rand_diff #>  #> $focal_comparison #> # A tibble: 2 × 6 #>   variable weight_index ci_lower ci_upper ci_method ci_type    #>   <chr>           <int>    <dbl>    <dbl> <chr>     <chr>      #> 1 Var7                1   -0.869   -0.813 basic     focal_diff #> 2 Var8                2   -0.865   -0.805 basic     focal_diff #>  # }"},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"rwa-010","dir":"Changelog","previous_headings":"","what":"rwa 0.1.0","title":"rwa 0.1.0","text":"CRAN release: 2025-07-16","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"rwa 0.1.0","text":"Bootstrap confidence intervals: Added bootstrap = TRUE parameter rwa() statistical significance testing relative weights Result sorting: Added sort = TRUE parameter automatically sort results importance (descending order). Set sort = FALSE preserve original predictor order Comprehensive vignette: New detailed documentation covering methodology, examples, best practices Enhanced documentation: Updated README function documentation","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"technical-improvements-0-1-0","dir":"Changelog","previous_headings":"","what":"Technical Improvements","title":"rwa 0.1.0","text":"Package compliance: Updated DESCRIPTION proper Authors@R field CRAN submission CI/CD: Enhanced GitHub Actions workflow vignette building support Dependencies: Added boot, purrr, utils packages bootstrap functionality Code quality improvements: Fixed long lines R code meet CRAN standards Documentation cleanup: Improved code formatting removed unused variables Enhanced vignette formatting: Cleaned formatting comprehensive vignette documentation","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"bug-fixes-0-1-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"rwa 0.1.0","text":"Fixed vignette compilation issues Resolved R CMD check warnings notes Removed unused variables eliminate R CMD check notes Improved consistency code formatting","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"version-0-1-0","dir":"Changelog","previous_headings":"","what":"Version 0.0.3","title":"rwa 0.1.0","text":"Re-submission CRAN Unwrap \\donttest{} examples unnecessary","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"version-0-1-0-1","dir":"Changelog","previous_headings":"","what":"Version 0.0.2","title":"rwa 0.1.0","text":"Re-submission CRAN DOI references added DESCRIPTION Added CodeFactor badge Typos DESCRIPTION rectified","code":""},{"path":"https://martinctc.github.io/rwa/news/index.html","id":"version-0-1-0-2","dir":"Changelog","previous_headings":"","what":"Version 0.0.1","title":"rwa 0.1.0","text":"First submission CRAN (required re-submit) rwa() plot_rwa() remove_all_na_cols() %>% operator exported","code":""}]
